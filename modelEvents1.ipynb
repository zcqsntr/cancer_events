{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim here is to train a supervised CNN to recognise duplications and deletions\n",
    "1) Generate some event data to classify\n",
    "2) Define the network\n",
    "3) Do some training \n",
    "4) Test the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff06ebf00b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "rng.seed(1001)\n",
    "torch.manual_seed(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Generate some data\n",
    "\n",
    "def data_generator(ndat,nL,sd):\n",
    "    data = np.zeros((ndat,nL)) \n",
    "    labs = np.zeros((ndat,)) \n",
    "    for i in range(0,ndat):\n",
    "        # choose dup or del\n",
    "        evnt = rng.choice([-1,1])\n",
    "        #labs[i,] = int( (evnt+1)/2 )\n",
    "        if evnt == -1:\n",
    "            labs[i,] = 0\n",
    "        else:\n",
    "            labs[i,] = 1\n",
    "    \n",
    "        # choose start\n",
    "        start = rng.randint(0,nL-1)\n",
    "        #print(start,\"\\t\",type)\n",
    "        #start = 4\n",
    "    \n",
    "        # create event and add noise\n",
    "        data[i, start:(start+2)] = evnt\n",
    "        data[i,] += rng.normal(0,sd,(nL))\n",
    "    \n",
    "        #print(data[i,])\n",
    "\n",
    "    return [data, labs]\n",
    "        \n",
    "ndat = 10000\n",
    "nL = 10 #length \n",
    "sd = 0.5 #sd of noise \n",
    "data, labs = data_generator(ndat, nL, sd)\n",
    "x_train = torch.from_numpy(data).float()\n",
    "y_train = torch.from_numpy(labs).long()\n",
    "#print(labs)\n",
    "#print(data[0:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Define a model. We will start with logistic regression\n",
    "class logisticRegression(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(logisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_dim, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        #out = F.softmax(self.linear(x),dim=1) # this is the dimension of the array\n",
    "        # softmax is not needed here as the cross-entropy loss handles\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model = logisticRegression(nL, 2)\n",
    "#print(model)\n",
    "#print(model(x_train[0:10,]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100], loss: 0.691040\n",
      "Epoch[2/100], loss: 0.726043\n",
      "Epoch[3/100], loss: 0.694123\n",
      "Epoch[4/100], loss: 0.722817\n",
      "Epoch[5/100], loss: 0.697034\n",
      "Epoch[6/100], loss: 0.710685\n",
      "Epoch[7/100], loss: 0.706742\n",
      "Epoch[8/100], loss: 0.715904\n",
      "Epoch[9/100], loss: 0.709402\n",
      "Epoch[10/100], loss: 0.662896\n",
      "Epoch[11/100], loss: 0.715847\n",
      "Epoch[12/100], loss: 0.721435\n",
      "Epoch[13/100], loss: 0.714630\n",
      "Epoch[14/100], loss: 0.740611\n",
      "Epoch[15/100], loss: 0.713838\n",
      "Epoch[16/100], loss: 0.685212\n",
      "Epoch[17/100], loss: 0.691550\n",
      "Epoch[18/100], loss: 0.693416\n",
      "Epoch[19/100], loss: 0.694904\n",
      "Epoch[20/100], loss: 0.700551\n",
      "Epoch[21/100], loss: 0.698162\n",
      "Epoch[22/100], loss: 0.693012\n",
      "Epoch[23/100], loss: 0.705024\n",
      "Epoch[24/100], loss: 0.692445\n",
      "Epoch[25/100], loss: 0.735523\n",
      "Epoch[26/100], loss: 0.724380\n",
      "Epoch[27/100], loss: 0.741035\n",
      "Epoch[28/100], loss: 0.706593\n",
      "Epoch[29/100], loss: 0.684846\n",
      "Epoch[30/100], loss: 0.687066\n",
      "Epoch[31/100], loss: 0.707014\n",
      "Epoch[32/100], loss: 0.740043\n",
      "Epoch[33/100], loss: 0.704833\n",
      "Epoch[34/100], loss: 0.667799\n",
      "Epoch[35/100], loss: 0.692975\n",
      "Epoch[36/100], loss: 0.686162\n",
      "Epoch[37/100], loss: 0.715634\n",
      "Epoch[38/100], loss: 0.702695\n",
      "Epoch[39/100], loss: 0.744743\n",
      "Epoch[40/100], loss: 0.668440\n",
      "Epoch[41/100], loss: 0.649729\n",
      "Epoch[42/100], loss: 0.686130\n",
      "Epoch[43/100], loss: 0.707628\n",
      "Epoch[44/100], loss: 0.703652\n",
      "Epoch[45/100], loss: 0.711389\n",
      "Epoch[46/100], loss: 0.687293\n",
      "Epoch[47/100], loss: 0.683959\n",
      "Epoch[48/100], loss: 0.696482\n",
      "Epoch[49/100], loss: 0.690462\n",
      "Epoch[50/100], loss: 0.679125\n",
      "Epoch[51/100], loss: 0.677192\n",
      "Epoch[52/100], loss: 0.714696\n",
      "Epoch[53/100], loss: 0.736134\n",
      "Epoch[54/100], loss: 0.694384\n",
      "Epoch[55/100], loss: 0.684016\n",
      "Epoch[56/100], loss: 0.696322\n",
      "Epoch[57/100], loss: 0.691347\n",
      "Epoch[58/100], loss: 0.712689\n",
      "Epoch[59/100], loss: 0.664686\n",
      "Epoch[60/100], loss: 0.697158\n",
      "Epoch[61/100], loss: 0.695107\n",
      "Epoch[62/100], loss: 0.635820\n",
      "Epoch[63/100], loss: 0.677498\n",
      "Epoch[64/100], loss: 0.678713\n",
      "Epoch[65/100], loss: 0.717955\n",
      "Epoch[66/100], loss: 0.690652\n",
      "Epoch[67/100], loss: 0.701190\n",
      "Epoch[68/100], loss: 0.671501\n",
      "Epoch[69/100], loss: 0.676704\n",
      "Epoch[70/100], loss: 0.653527\n",
      "Epoch[71/100], loss: 0.697161\n",
      "Epoch[72/100], loss: 0.684104\n",
      "Epoch[73/100], loss: 0.666700\n",
      "Epoch[74/100], loss: 0.675834\n",
      "Epoch[75/100], loss: 0.725957\n",
      "Epoch[76/100], loss: 0.685005\n",
      "Epoch[77/100], loss: 0.690360\n",
      "Epoch[78/100], loss: 0.680942\n",
      "Epoch[79/100], loss: 0.719077\n",
      "Epoch[80/100], loss: 0.694977\n",
      "Epoch[81/100], loss: 0.678632\n",
      "Epoch[82/100], loss: 0.670172\n",
      "Epoch[83/100], loss: 0.650251\n",
      "Epoch[84/100], loss: 0.671294\n",
      "Epoch[85/100], loss: 0.675071\n",
      "Epoch[86/100], loss: 0.681321\n",
      "Epoch[87/100], loss: 0.703667\n",
      "Epoch[88/100], loss: 0.684148\n",
      "Epoch[89/100], loss: 0.662689\n",
      "Epoch[90/100], loss: 0.656299\n",
      "Epoch[91/100], loss: 0.703987\n",
      "Epoch[92/100], loss: 0.728089\n",
      "Epoch[93/100], loss: 0.692227\n",
      "Epoch[94/100], loss: 0.720304\n",
      "Epoch[95/100], loss: 0.689446\n",
      "Epoch[96/100], loss: 0.710760\n",
      "Epoch[97/100], loss: 0.664907\n",
      "Epoch[98/100], loss: 0.694466\n",
      "Epoch[99/100], loss: 0.675981\n",
      "Epoch[100/100], loss: 0.672407\n",
      "tensor([[-0.0142, -0.0505],\n",
      "        [ 0.1632, -0.0035],\n",
      "        [-0.2740, -0.1871],\n",
      "        [-0.2492,  0.0391],\n",
      "        [-0.1588, -0.3171],\n",
      "        [ 0.2743,  0.1062],\n",
      "        [-0.1771, -1.0524],\n",
      "        [-0.4512,  0.2444],\n",
      "        [ 0.2777,  0.1646],\n",
      "        [ 0.2528, -0.3427],\n",
      "        [-0.3522, -0.8880],\n",
      "        [-0.2870, -0.2281],\n",
      "        [-0.1608, -0.2911],\n",
      "        [-0.3037, -0.1403],\n",
      "        [ 0.0891,  0.0768],\n",
      "        [ 0.6422, -0.1222],\n",
      "        [-0.0818, -0.1981],\n",
      "        [-0.8309, -0.4772],\n",
      "        [ 0.2104,  0.0951],\n",
      "        [-0.3152, -0.2060],\n",
      "        [ 0.2400,  0.0233],\n",
      "        [ 0.3815,  0.1553],\n",
      "        [-0.3630, -0.4768],\n",
      "        [-0.2764, -0.8400],\n",
      "        [-0.1850, -0.2653],\n",
      "        [-0.0242, -0.2516],\n",
      "        [-1.2023, -1.1090],\n",
      "        [-0.2939, -0.7791],\n",
      "        [ 0.5832, -0.0067],\n",
      "        [-0.2817, -0.5434],\n",
      "        [ 0.7839,  0.4868],\n",
      "        [ 0.0269, -0.2149],\n",
      "        [ 0.7346,  0.0845],\n",
      "        [ 1.2479,  0.0606],\n",
      "        [-0.3481, -0.5172],\n",
      "        [-0.0548, -0.1963],\n",
      "        [ 0.2636,  0.3056],\n",
      "        [-0.0952, -0.2127],\n",
      "        [-0.0875, -0.7027],\n",
      "        [-0.2006,  0.0407],\n",
      "        [-0.2247, -0.3525],\n",
      "        [ 0.1146, -0.1070],\n",
      "        [-0.0113, -0.2310],\n",
      "        [ 0.0428,  0.1899],\n",
      "        [ 0.1770,  0.0476],\n",
      "        [-0.3620, -0.5589],\n",
      "        [-0.4815, -0.2088],\n",
      "        [ 0.0724, -0.2846],\n",
      "        [-0.3415,  0.1501],\n",
      "        [ 0.9549, -0.0877],\n",
      "        [-0.0085, -0.2408],\n",
      "        [ 0.0366, -0.1507],\n",
      "        [-0.3122, -0.3122],\n",
      "        [-0.1006, -0.4045],\n",
      "        [ 0.6775, -0.0649],\n",
      "        [-0.2329, -0.2311],\n",
      "        [-0.7019, -0.3487],\n",
      "        [-1.0853, -0.5953],\n",
      "        [-0.3865, -0.5119],\n",
      "        [ 0.1121,  0.1181],\n",
      "        [-0.8247, -0.6212],\n",
      "        [ 0.2356,  0.4872],\n",
      "        [-0.0132, -0.3179],\n",
      "        [-0.6797, -0.7717],\n",
      "        [-0.0926, -0.3375],\n",
      "        [-0.1661,  0.2293],\n",
      "        [ 0.1828,  0.1562],\n",
      "        [-0.0172, -0.3549],\n",
      "        [ 0.4402,  0.3892],\n",
      "        [-0.2894, -0.7100],\n",
      "        [-0.2731, -0.1221],\n",
      "        [-0.3973, -0.4780],\n",
      "        [-0.4726, -0.4287],\n",
      "        [ 0.1109, -0.3964],\n",
      "        [-0.5001, -0.9644],\n",
      "        [-0.0312,  0.2154],\n",
      "        [-0.1712, -0.4874],\n",
      "        [ 0.0876, -0.1626],\n",
      "        [-0.9294, -0.4170],\n",
      "        [ 0.3467, -0.2325],\n",
      "        [ 0.1954,  0.2131],\n",
      "        [-0.4559, -0.3093],\n",
      "        [ 0.5971,  0.1720],\n",
      "        [-0.2159, -0.4249],\n",
      "        [ 0.1232, -0.2533],\n",
      "        [ 0.5549,  0.0587],\n",
      "        [-0.2965, -0.2839],\n",
      "        [ 0.6470, -0.2381],\n",
      "        [ 0.2433,  0.0794],\n",
      "        [ 0.1377, -0.3265],\n",
      "        [ 0.5105,  0.6086],\n",
      "        [ 0.6030,  0.4788],\n",
      "        [ 0.5808, -0.1412],\n",
      "        [-0.2287, -0.9069],\n",
      "        [-0.2896, -0.3558],\n",
      "        [ 0.0460,  0.0859],\n",
      "        [-0.7848, -0.6511],\n",
      "        [ 0.3901,  0.1151],\n",
      "        [ 0.0766,  0.3200],\n",
      "        [ 0.1464, -0.1691]], grad_fn=<AddmmBackward>)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 3) Do some training\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "num_epochs = int(ndat/batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "    #inds = rng.choice(range(0,ndat),size=batch_size, replace=False)\n",
    "    inds = range(i,i+batch_size)\n",
    "    inputs = x_train[inds,]\n",
    "    target = y_train[inds,]\n",
    "\n",
    "    i += batch_size\n",
    "    \n",
    "    # forward\n",
    "    out = model(inputs)\n",
    "    #print(target.shape)\n",
    "    #print(out.shape)\n",
    "    loss = criterion(out, target)\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch[{epoch+1}/{num_epochs}], loss: {loss.item():.6f}')\n",
    "print(out)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Test the model prediction\n",
    "#outputs = model(x_train[0:10,])\n",
    "#print(outputs)\n",
    "#print(labs[0:10])\n",
    "#_, predicted = torch.max(outputs.data, 1)\n",
    "#print(predicted, labs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 55 %\n"
     ]
    }
   ],
   "source": [
    "ntest = 1000\n",
    "tdata, tlabs = data_generator(ntest, nL, sd)\n",
    "x_test = torch.from_numpy(tdata).float()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    #print( abs(predicted-labs) )\n",
    "    incorrect = (abs(predicted-tlabs)).sum()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * (ntest-incorrect) / float(ntest)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight tensor([[-0.0730, -0.2251,  0.2118, -0.2222, -0.2056, -0.1443, -0.1919,  0.0838,\n",
      "          0.2506, -0.1956],\n",
      "        [-0.2334, -0.0082, -0.1015,  0.0927, -0.2110, -0.0133, -0.2665,  0.1894,\n",
      "          0.0745, -0.2849]])\n",
      "linear.bias tensor([ 0.0026, -0.2017])\n"
     ]
    }
   ],
   "source": [
    "# examine the parameters of the fitted model\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
